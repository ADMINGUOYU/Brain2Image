# This is the EEG-fMRI alignment model
# We use the embeddings generated by eeg_encoder (CBraMod back-bone for now)
# We then use a transformer module, take EEG embeddings as key and value,
# and fMRI embeddings as query, to learn the alignment between the two modalities

# import torch
import torch
import torch.nn as nn
import torch.nn.functional as F

# import the transformer module
from .layers import MultiHeadAttention

# import the eeg encoder
from .encoders.cbramod_eeg_encoder import CBraMod_EEG_Encoder as EEG_Encoder

# import type hinting
import typing

# Define the EEG-fMRI alignment model
class EEG_fMRI_Align(nn.Module):

    """
    EEG_fMRI_Align
    """

    def __init__(self, param: typing.Dict):

        """
        Initialize EEG_fMRI_Align model.
        Args:
            param: Configuration object containing model parameters.
            - param['EEG_Encoder'] Dict: Configuration for the EEG encoder (CBraMod_EEG_Encoder)
            - param['Attention_Merge'] Dict: Configuration for the attention module used for merging EEG and fMRI embeddings
                - param['Attention_Merge']['alignment_attention_heads']: Number of attention heads for the alignment transformer (default 8)
                - param['Attention_Merge']['alignment_attention_dropout']: Dropout rate for the alignment transformer (default 0.1)
            - param['Loss'] Dict: pending for loss function configuration (if needed)
                - param['Loss']['mse_scale']: Scaling factor for MSE loss (default 1.0)
                - param['Loss']['infonce_scale']: Scaling factor for InfoNCE loss (default 1.0)
                - param['Loss']['proto_distill_scale']: Scaling factor for prototypical distillation loss (default 1.0)
                - param['Loss']['temperature']: Temperature for InfoNCE loss (default 0.07)
                - param['Loss']['normalize_fmri']: Whether to normalize fMRI embeddings to match the range of InfoNCE loss (default False)
        """
        super(EEG_fMRI_Align, self).__init__()

        # Check for required parameters
        if 'EEG_Encoder' not in param:
            raise ValueError("Missing 'EEG_Encoder' configuration in param.")
        if 'Attention_Merge' not in param:
            raise ValueError("Missing 'Attention_Merge' configuration in param.")
        if 'Loss' not in param:
            print("Warning: 'Loss' configuration not found in param. Using default loss parameters.")

        # EEG encoder
        self.eeg_encoder = EEG_Encoder(param['EEG_Encoder'])

        # Transformer module for alignment
        self.transformer = MultiHeadAttention(
            d_model = self.eeg_encoder.output_dim, 
            num_heads = param['Attention_Merge'].get('alignment_attention_heads', 8),
            dropout = param['Attention_Merge'].get('alignment_attention_dropout', 0.1)
        )

        # Loss function parameters
        self.mse_scale = param.get('Loss', {}).get('mse_scale', 1.0)
        self.infonce_scale = param.get('Loss', {}).get('infonce_scale', 1.0)
        self.proto_distill_scale = param.get('Loss', {}).get('proto_distill_scale',  1.0)
        self.temperature = param.get('Loss', {}).get('temperature', 0.07)
        self.normalize_fmri = param.get('Loss', {}).get('normalize_fmri', True)

    def forward(self, 
                eeg_input: torch.Tensor, 
                fmri_input: torch.Tensor) -> torch.Tensor:
        
        # TODO: confirm eeg_input and fmri_input shapes, 
        # Assuming different shapes.
        # eeg_input: (batch, 63, 1, 200) -> (batch, embedding_dim) after EEG encoder
        # fmri_input: (batch, fmri_seq_len, fmri_feature_dim)
        
        # Encode EEG input
        eeg_embeds = self.eeg_encoder.forward(eeg_input)  # (batch, embedding_dim)
        # we assume is (batch, 4096)

        # Use fMRI input as query and EEG embeddings as key and value
        # key, query shape (batch, 4096)
        # we reshape it to (batch, 1, 4096) to be compatible with the attention module
        eeg_embeds = eeg_embeds.unsqueeze(1)  # (batch, 1, embedding_dim)
        aligned_embeds, _ = self.transformer.forward(query = fmri_input,
                                                    key = eeg_embeds,
                                                    value = eeg_embeds)  # (batch, embedding_dim)
        aligned_embeds = F.normalize(aligned_embeds, dim=-1)  # unit-norm output
        return aligned_embeds

    def save_model(self, path: str):
        # save the state dict
        # we save CBraMod backbone and others separately
        
        # get self.eeg_encoder state dict
        eeg_encoder_state_dict = self.eeg_encoder.state_dict()

        # get self.transformer state dict
        transformer_state_dict = self.transformer.state_dict()

        # get self.parameters
        parameters = {
            'mse_scale': self.mse_scale,
            'infonce_scale': self.infonce_scale,
            'proto_distill_scale': self.proto_distill_scale,
            'temperature': self.temperature,
            'normalize_fmri': self.normalize_fmri
        }

        # save both state dicts
        torch.save({
            'eeg_encoder': eeg_encoder_state_dict,
            'transformer': transformer_state_dict,
            'parameters': parameters
        }, path)

    def load_model(self, path: str, device: torch.device):
        loaded = torch.load(path, map_location=device)
        self.eeg_encoder.load_state_dict(loaded['eeg_encoder'])
        self.transformer.load_state_dict(loaded['transformer'])
        self.mse_scale = loaded['parameters']['mse_scale']
        self.infonce_scale = loaded['parameters']['infonce_scale']
        self.proto_distill_scale = loaded['parameters']['proto_distill_scale']
        self.temperature = loaded['parameters']['temperature']
        self.normalize_fmri = loaded['parameters']['normalize_fmri']

    def calc_alignment_loss(self, 
                            aligned_embeds: torch.Tensor, 
                            target_embeds: torch.Tensor,
                            label: torch.Tensor) -> torch.Tensor:
        
        """
        Args
            aligned_embeds: Aligned EEG embeddings output by the model (batch, embedding_dim)
            target_embeds: Target fMRI embeddings (batch, embedding_dim)
            label: Clustered category labels for each sample (batch,)
        Returns
            total_loss: Combined loss for EEG-fMRI alignment
            mse_loss: Mean Squared Error loss component
            infonce_loss: InfoNCE loss component
            proto_loss: Prototypical distillation loss component
        """

        # Normalize target embeddings to unit sphere if configured
        if self.normalize_fmri:
            target_embeds = F.normalize(target_embeds, dim=1)

        # MSE loss: sum over feature dim, mean over batch → values in [0, 4] for unit-norm vectors
        mse_loss = F.mse_loss(aligned_embeds, target_embeds, reduction='none').sum(dim=1).mean()

        # InfoNCE loss (aligned_embeds already unit-norm from forward; target_embeds normalized above)
        logits = torch.matmul(aligned_embeds, target_embeds.T) / self.temperature
        labels = torch.arange(aligned_embeds.size(0), device=aligned_embeds.device)
        infonce_loss = F.cross_entropy(logits, labels)

        # Prototypical distillation loss
        # Adapted from MultiEYE/process/finetune.py  (prototypical distill block)
        #
        # For every class c present in this batch, compute the mean embedding
        # (prototype) of aligned_embeds and target_embeds separately, then
        # penalize their MSE. 
        # This encourages the EEG representation space to organize its 
        # class-level geometry in the same way as fMRI.
        #
        # How this is mapped to the fundus/OCT setting in MultiEYE:
        # aligned_embeds  ->  f_dist  (fundus prototype, shape: [n_cls, emb_dim])
        # target_embeds   ->  o_dist  (OCT prototype, shape: [n_cls, emb_dim])

        exist_classes = torch.unique(label) # classes present in batch
        n_cls = exist_classes.shape[0]
        emb_dim = aligned_embeds.shape[1]
        device = aligned_embeds.device

        # Accumulators for prototypes and sample counts per class
        eeg_proto  = torch.zeros(n_cls, emb_dim, device = device)   # f_dist
        fmri_proto = torch.zeros(n_cls, emb_dim, device = device)   # o_dist
        counts     = torch.zeros(n_cls,          device = device)   # f_num / o_num

        for idx, cls in enumerate(exist_classes):
            mask = (label == cls)                    # boolean mask for this class
            eeg_proto[idx]  = aligned_embeds[mask].mean(dim = 0)
            fmri_proto[idx] = target_embeds[mask].mean(dim = 0)
            counts[idx]     = mask.sum()

        # MSE between EEG and fMRI prototypes (re-normalize after mean to restore unit sphere)
        eeg_proto  = F.normalize(eeg_proto,  dim=1)
        fmri_proto = F.normalize(fmri_proto, dim=1)
        proto_loss = F.mse_loss(eeg_proto, fmri_proto, reduction='none').sum(dim=1).mean()

        # Combined loss
        total_loss = self.mse_scale * mse_loss + self.infonce_scale * infonce_loss + self.proto_distill_scale * proto_loss

        return total_loss, mse_loss, infonce_loss, proto_loss
    
    def get_metrics_for_alignment(self, 
                                aligned_embeds: torch.Tensor, 
                                target_embeds: torch.Tensor) -> typing.Tuple[float, float, float]:
        
        """
        Compute evaluation metrics for EEG-fMRI alignment.
        Args:
            aligned_embeds: Aligned EEG embeddings output by the model (batch, embedding_dim)
            target_embeds: Target fMRI embeddings (batch, embedding_dim)
        Returns:
            mse: Mean Squared Error between aligned EEG embeddings and target fMRI embeddings
            cos_sim: Average cosine similarity between aligned EEG embeddings and target fMRI embeddings
            retrieval_acc: Top-1 retrieval accuracy based on cosine similaritys
        """
        # Normalize target embeddings to unit sphere if configured
        if self.normalize_fmri:
            target_embeds = F.normalize(target_embeds, dim=1)

        # MSE: sum over feature dim, mean over batch → consistent with training loss
        mse = F.mse_loss(aligned_embeds, target_embeds, reduction='none').sum(dim=1).mean().item()

        # Cosine similarity (aligned_embeds already unit-norm)
        cos_sim = F.cosine_similarity(aligned_embeds, target_embeds, dim=1).mean().item()

        # Top-1 retrieval accuracy
        pred_norm = F.normalize(aligned_embeds, dim=1)
        target_norm = F.normalize(target_embeds, dim=1)
        sim_matrix = torch.matmul(pred_norm, target_norm.T)
        top_1 = torch.argmax(sim_matrix, dim=1)
        retrieval_acc = (top_1 == torch.arange(len(aligned_embeds), device=aligned_embeds.device)).float().mean().item()

        return mse, cos_sim, retrieval_acc

# Test code
if __name__ == "__main__":
    # Create dummy input
    batch_size = 4
    fmri_seq_len = 1
    fmri_feature_dim = 4096
    dummy_eeg_input = torch.randn(batch_size, 63, 1, 200)
    dummy_fmri_input = torch.randn(batch_size, fmri_seq_len, fmri_feature_dim)
    dummy_label = torch.tensor([0, 1, 0, 1])  # Example class labels for prototypical distillation loss

    # Create dummy param object
    param = {
        'EEG_Encoder': {
            'pooling_type': 'multitoken_vit',  # 'flatten', 'attention', or 'multitoken_vit'
            'num_tokens': 4,
            'num_transformer_layers': 4,
            'num_attention_heads': 4,
            'use_pretrained_weights': False
        },
        'Attention_Merge': {
            'alignment_attention_heads': 8
        },
        'Loss': {
            'mse_scale': 1.0,
            'infonce_scale': 1.0,
            'proto_distill_scale': 1.0,
            'temperature': 0.07,
            'normalize_fmri': True
        }
    }

    # Create model instance
    model = EEG_fMRI_Align(param)

    # Forward pass
    output = model(dummy_eeg_input, dummy_fmri_input)
    print("Output shape:", output.shape)

    # Test Loss (pred -> output, target -> dummy_fmri_input, label -> dummy_label)
    # squeeze first
    dummy_fmri_input = dummy_fmri_input.squeeze(1)  # (batch, fmri_feature_dim)
    output = output.squeeze(1)  # (batch, embedding_dim)
    loss, mse_loss, infonce_loss, proto_loss = model.calc_alignment_loss(output, dummy_fmri_input, dummy_label)
    print("Total Loss:", loss.item())
    print("MSE Loss:", mse_loss.item())
    print("InfoNCE Loss:", infonce_loss.item())
    print("Proto Distill Loss:", proto_loss.item())