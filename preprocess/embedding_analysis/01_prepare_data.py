# This is the first step of the embedding analysis pipeline. 
# It prepares the data for the subsequent steps.
# We get repack NSD dataset with the 4096 embeddings
# generated by the MindEYE2 shared latent MLP.

# NOTE: Please run datasets/nsd-fmri_repack.py
#       with all subjects first to get the initial dataframe
# Initial dataframe columns:
# (fmri / image_index / subject / split / sample_id / tar_path).
# Out dataframe:
# (fmri_embedding / image_index / subject / split / sample_id / tar_path)

# Import necessary libraries
import os
import torch
import pandas as pd
from tqdm import tqdm

# --------------- Start of configuration --------------- #

# Paths to necessary files
processed_dir = "datasets/processed"

# Saving directory
processed_dir_for_analysis = "datasets/processed/embedding_analysis"
os.makedirs(processed_dir_for_analysis, exist_ok = True)

# The subjects we want to use 
# NOTE: in (nsd subject)
# NOTE: we LOOP every combination of subjects
subjects =  ['sub-01', 'sub-02', 'sub-05', 'sub-07']

# Device for processing
CUDA = 0
device = f'cuda:{CUDA}' if torch.cuda.is_available() else 'cpu'

# Make a copy of full ckpt? (downloaded from MindEYE2)
# You might need this if you wish to load the full ckpt in end-to-end model
# saved under: <processed_di>/mindeye2/sub-<subject_number>_last_full.pth
preserve_full_ckpt = True

# ---------------- End of configuration ---------------- #

# First we have to download the ckpt from MindEYE2
# TODO: safe to call if already downloaded, will return the path if so.
def download_ckpt(subject_number: int):
    # construct URL (make sure subject number in TWO digit)
    ckpt_url = f"https://huggingface.co/datasets/pscotti/mindeyev2/resolve/main/train_logs/final_subj{subject_number:02d}_pretrained_40sess_24bs/last.pth?download=true"
    ckpt_path = processed_dir + f"/mindeye2/sub-{subject_number:02d}_last.pth"
    if not os.path.exists(ckpt_path):
        print(f"Downloading MindEYE2 checkpoint for subject {subject_number}...")
        # create parent directory if it doesn't exist
        os.makedirs(os.path.dirname(ckpt_path), exist_ok = True)
        # use wget to download the file
        os.system(f"wget -O {ckpt_path} {ckpt_url}")
        # If specified
        if preserve_full_ckpt:
            # we make a copy of the full ckpt
            full_ckpt_path = processed_dir + f"/mindeye2/sub-{subject_number:02d}_last_full.pth"
            os.system(f"cp {ckpt_path} {full_ckpt_path}")
        # Opens the file and only keep ridge keys to save space
        checkpoint = torch.load(ckpt_path, map_location = 'cpu')
        ridge_state_dict = {k: v for k, v in checkpoint['model_state_dict'].items() if 'ridge' in k}
        torch.save({'model_state_dict': ridge_state_dict}, ckpt_path)
        print(f"Checkpoint downloaded and saved to {ckpt_path}.")
    else:
        print(f"Checkpoint for subject {subject_number} already exists at {ckpt_path}.")
    return ckpt_path
# Now, we download
mindeye2_ckpts = []
for nsd_subject in subjects:
    mindeye2_ckpts.append(download_ckpt(int(nsd_subject[-2:])))

# Now we loop through the paired data and construct the dataset
for nsd_subject, mindeye2_ckpt in zip(subjects, mindeye2_ckpts):
    print(f"\nProcessing nsd subject {nsd_subject}")

    # Load nsd fMRI data from dataframes
    nsd_fmri_data_df_path = None
    for file in os.listdir(processed_dir):
        if file.startswith('nsd_fmri_data_df_') and file.endswith('.pkl'):
            if nsd_subject in file:
                nsd_fmri_data_df_path = os.path.join(processed_dir, file)
                break
    if nsd_fmri_data_df_path is None:
        raise FileNotFoundError(f"Could not find the nsd_fmri_data_df file for subject {nsd_subject} in {processed_dir}.")
    nsd_fmri_data_df: pd.DataFrame = pd.read_pickle(nsd_fmri_data_df_path)
    nsd_fmri_data_df = nsd_fmri_data_df[nsd_fmri_data_df['subject'] == nsd_subject]
    print(f"Loaded NSD fMRI data with {len(nsd_fmri_data_df)} samples.")

    # Get the fMRI data as a list of numpy arrays
    fmri_data_list = nsd_fmri_data_df['fmri'].tolist()

    # Now we create the linear layer for fmri
    # First we load the ckpt and get the ridge regression parameters
    checkpoint = torch.load(mindeye2_ckpt, map_location = device)
    ridge_state_dict = checkpoint['model_state_dict']
    # Keys containing 'ridge': ['ridge.linears.0.weight', 'ridge.linears.0.bias']
    # we only needs these two parameters for the linear layer
    ridge_weight = ridge_state_dict['ridge.linears.0.weight']
    ridge_bias = ridge_state_dict['ridge.linears.0.bias']
    print(f"Loaded ridge regression parameters from checkpoint: weight shape {ridge_weight.shape}, bias shape {ridge_bias.shape}.")
    # We infer the input and output dimension from the weight shape
    input_dim = ridge_weight.shape[1]
    output_dim = ridge_weight.shape[0]
    print(f"Inferred ridge regression input dimension: {input_dim}, output dimension: {output_dim}.")
    # assert
    assert input_dim == fmri_data_list[0].shape[1], f"Input dimension of ridge regression ({input_dim}) does not match fMRI data dimension ({fmri_data_list[0].shape[1]})."
    # We create the torch linear layer and set its parameters
    projection_layer = torch.nn.Linear(input_dim, output_dim)
    with torch.no_grad():
        projection_layer.weight.copy_(ridge_weight)
        projection_layer.bias.copy_(ridge_bias)
        projection_layer.to(device)
        print("Created projection layer and loaded ridge regression parameters.")
    # We do the forward pass to get the hidden features for each fmri data
    fmri_data_hidden_list = []
    for fmri_data in tqdm(fmri_data_list, desc = "Processing fMRI data"):
        fmri_tensor = torch.from_numpy(fmri_data).float().to(device)
        with torch.no_grad():
            hidden_features = projection_layer(fmri_tensor).cpu().numpy()
        fmri_data_hidden_list.append(hidden_features)
    print(f"Processed fMRI data to hidden features with shape {fmri_data_hidden_list[0].shape}.")
    # Assign back to fmri_data_list
    del fmri_data_list  # free up memory
    fmri_data_list = fmri_data_hidden_list

    # Now we save the new dataframe with fmri_embedding column
    nsd_fmri_embedding_df = nsd_fmri_data_df.copy()
    nsd_fmri_embedding_df['fmri_embedding'] = fmri_data_list
    # We drop the original fmri data column to save space
    nsd_fmri_embedding_df = nsd_fmri_embedding_df.drop(columns = ['fmri'])
    # We save the new dataframe to a new pickle file
    nsd_fmri_embedding_df_path = os.path.join(processed_dir_for_analysis, f"nsd_fmri_analysis_df_{nsd_subject}.pkl")
    nsd_fmri_embedding_df.to_pickle(nsd_fmri_embedding_df_path)
    print(f"Saved new dataframe with fMRI embeddings to {nsd_fmri_embedding_df_path}.")
    